{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada94ac5",
   "metadata": {},
   "source": [
    "# ChemTab (PCDNN_V2) Benchmark Notebook\n",
    "### This notebook contains experiments around the different parameters for the model\n",
    "#### Amol Salunkhe\n",
    "\n",
    "Develop a deep neural architecture ChemTab that jointly optimizes two neural networks for the tasks of reduced basis learning (encoder) and reverse lookup function learning (regressor). The first network (Shallow AutoEncoder) will focus on linear dimensionality reduction and create a linear embedding for the source species. The second network (Deep Regressor) will focus on learning a lookup function from the linear embedding to the Thermo-chemical state variables.\n",
    "\n",
    "### Overall Problem Formulation\n",
    "\n",
    "\\begin{equation}\\label{eqn:lossfunc}\n",
    "    \\begin{aligned}\n",
    "    \\textrm{min} \\quad \\mathcal{L}_{joint} = \\lambda_{encoder} * \\mathcal{L}_{encoder} + \\lambda_{regressor}* \\mathcal{L}_{regressor} \n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "### Reduced Basis Problem Formulation (Encoder)\n",
    "\n",
    "\\begin{equation}\\label{eqn:Reduced-Basis-Learning}\n",
    "    \\begin{aligned}\n",
    "    \\def\\sss{\\scriptscriptstyle}\n",
    "    {}\n",
    "    \\def\\stacktype{L}\n",
    "        {\\mathrm{Y^{'}}}{\\sss n\\times k} =  {Y}{\\sss n\\times s} \\times {W}{\\sss s\\times k} \\\\\n",
    "        \\textrm{s.t.,} \\quad k \\ll s\\\\\n",
    "        \\mathrm{\\phi}(Y) \\approx \\mathrm{\\phi}(Y^{'})\\\\\n",
    "        \\textrm{where, W is the Reduced Basis / encoder} \\\\\n",
    "        \\mathrm{\\phi} \\quad \\textrm{is the Reverse Lookup function / regressor}\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Lookup Learning Problem Formulation (Regressor)\n",
    "\n",
    "\\begin{equation}\\label{eqn:Reverse-Lookup-Learning}\n",
    "    \\begin{aligned}\n",
    "        \\textrm{min} \\quad ||{S_i} - \\mathrm{\\phi}(Y^{'}_i)||_{p}\\\\\n",
    "        \\textrm{s.t.,} \\quad p \\in r \\\\\n",
    "        \\quad k \\ll s\\\\\n",
    "        \\mathrm{\\phi}(Y) \\approx \\mathrm{\\phi}(Y^{'})\\\\\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2785986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn import gaussian_process\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import scipy.optimize\n",
    "from sklearn.utils.optimize import _check_optimize_result\n",
    "from sklearn.gaussian_process.kernels import Matern, RBF, WhiteKernel, RationalQuadratic, ExpSineSquared\n",
    "import time\n",
    "from sklearn.decomposition import PCA, SparsePCA\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers, activations, initializers, constraints, Sequential\n",
    "from tensorflow.keras.constraints import UnitNorm, Constraint\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fd6c67",
   "metadata": {},
   "source": [
    "## Network Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "91134051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patient early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=45)\n",
    "from tensorflow.keras import regularizers, activations, initializers, constraints, Sequential\n",
    "from tensorflow.keras.constraints import UnitNorm, Constraint\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class WeightsOrthogonalityConstraint (Constraint):\n",
    "    def __init__(self, encoding_dim, weightage = 1.0, axis = 0):\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.weightage = weightage\n",
    "        self.axis = axis\n",
    "        \n",
    "    def weights_orthogonality(self, w):\n",
    "        if(self.axis==1):\n",
    "            w = tf.transpose(w)\n",
    "        if(self.encoding_dim > 1):\n",
    "            m = tf.matmul(tf.transpose(w), w) - tf.eye(self.encoding_dim)\n",
    "            return self.weightage * tf.math.sqrt(tf.math.reduce_sum(tf.math.square(m)))\n",
    "        else:\n",
    "            m = tf.math.reduce_sum(w ** 2) - 1.\n",
    "            return m\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return self.weights_orthogonality(w)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'axis':self.axis,'weightage':self.weightage,'encoding_dim':self.encoding_dim}\n",
    "    \n",
    "\n",
    "class UncorrelatedFeaturesConstraint (Constraint):\n",
    "\n",
    "    def __init__(self, encoding_dim, weightage=1.0):\n",
    "        self.encoding_dim = encoding_dim\n",
    "        \n",
    "        self.weightage = weightage\n",
    "\n",
    "        self.covariance = None\n",
    "        \n",
    "    def get_covariance(self, x):\n",
    "        x_centered_list = []\n",
    "\n",
    "        for i in range(self.encoding_dim):\n",
    "            x_centered_list.append(x[:, i] - tf.math.reduce_mean(x[:, i]))\n",
    "\n",
    "        x_centered = tf.stack(x_centered_list)\n",
    "        \n",
    "        covariance = tf.matmul(x_centered, tf.transpose(x_centered)) / tf.cast(x_centered.get_shape()[0], tf.float32)\n",
    "        #covariance = tf.matmul(x_centered, tf.transpose(x_centered)) / tf.cast(tf.shape(x_centered)[0], tf.float32)\n",
    "\n",
    "        return covariance\n",
    "\n",
    "    # Constraint penalty\n",
    "    def uncorrelated_feature(self, x):\n",
    "        if(self.encoding_dim <= 1):\n",
    "            return 0.0\n",
    "        else:\n",
    "            output = tf.math.reduce_sum(tf.math.square(\n",
    "                self.covariance - tf.math.multiply(self.covariance, tf.eye(self.encoding_dim))))\n",
    "            return output\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.covariance = self.get_covariance(x)\n",
    "        return self.weightage * self.uncorrelated_feature(x)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'weightage': self.weightage, 'encoding_dim':self.encoding_dim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "306dc14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeError (Y_pred, Y_test):\n",
    "    evaluation_df_1 = pd.DataFrame()\n",
    "\n",
    "    evaluation_df_1['souener'] = Y_test.flatten()\n",
    "\n",
    "    evaluation_df_1['souener_pred'] = Y_pred.flatten()\n",
    "\n",
    "    evaluation_df_1['souener_pred_L1'] = evaluation_df_1['souener'] - evaluation_df_1['souener_pred'] \n",
    "\n",
    "    evaluation_df_1['souener_pred_L2'] = evaluation_df_1['souener_pred_L1'] * evaluation_df_1['souener_pred_L1']\n",
    "    \n",
    "    evaluation_df_1['souener_pred_L1Percent'] = ((evaluation_df_1['souener'] - evaluation_df_1['souener_pred'])/evaluation_df_1['souener']) \n",
    "    \n",
    "    TotalAbsoluteError = evaluation_df_1['souener_pred_L1'].abs().sum()\n",
    "\n",
    "    TotalSquaredError = evaluation_df_1['souener_pred_L2'].abs().sum()\n",
    "    \n",
    "    MeanAbsoluteError = evaluation_df_1['souener_pred_L1'].abs().sum()/evaluation_df_1['souener_pred_L1'].abs().count()\n",
    "\n",
    "    MeanSquaredError = evaluation_df_1['souener_pred_L2'].abs().sum()/evaluation_df_1['souener_pred_L2'].abs().count()\n",
    "    \n",
    "    NumPoints = evaluation_df_1['souener_pred_L1Percent'].abs().count()\n",
    "    \n",
    "    MeanPercentageError = evaluation_df_1['souener_pred_L1Percent'].abs().sum()/NumPoints\n",
    "    \n",
    "    return [TotalAbsoluteError,TotalSquaredError,MeanAbsoluteError,MeanSquaredError,MeanPercentageError,NumPoints]\n",
    "\n",
    "def printError (err):\n",
    "    TotalAbsoluteError = err[0]\n",
    "\n",
    "    TotalSquaredError = err[1]\n",
    "\n",
    "    MeanAbsoluteError = err[2]\n",
    "\n",
    "    MeanSquaredError = err[3]\n",
    "\n",
    "    MeanPercentageError = err[4]\n",
    "\n",
    "    NumPoints = err[5]\n",
    "    print ('Total Absolute Error: ', TotalAbsoluteError)\n",
    "    print ('Mean Absolute Error: ', MeanAbsoluteError)\n",
    "    print ('Mean Percentage Error: ', MeanPercentageError)\n",
    "    print ('Total Squared Error: ', TotalSquaredError)\n",
    "    print ('Mean Squared Error: ', MeanSquaredError)\n",
    "    print ('Number of Points: ', NumPoints)\n",
    "\n",
    "def computeAndPrintError(Y_pred, Y_test):    \n",
    "    err = computeError (Y_pred, Y_test)\n",
    "    printError (err)\n",
    "    return err\n",
    "\n",
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error [Souener]')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8b4161",
   "metadata": {},
   "source": [
    "## Data Loading & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "961fbba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data into a dataframe\n",
    "df = pd.read_csv('~/rom_project2/NewData_flames_data_with_L1_L2_errors_CH4-AIR_with_trimming.txt')\n",
    "\n",
    "#create an integer representation of the flame-id and add to the data frame\n",
    "df['flame_key_int'] = df[' flame_key'].mul(10000000).astype(int)\n",
    "\n",
    "#create an integer representation of the flame-id and add to the data frame\n",
    "df['X_int'] = df['X'].mul(10000000).astype(int)\n",
    "\n",
    "#create an integer to determine if the flame is included by the framework in the manifold creation and reverselookup\n",
    "#framework_untrimmed_flameids = [0.00115982, 0.00122087, 0.00128512, 0.00135276, 0.00142396, 0.0014989, 0.00157779, 0.00166083, 0.00174825, 0.00184026, 0.00193711, 0.00203907, 0.00214639, 0.00225936, 0.00237827, 0.01]\n",
    "\n",
    "framework_untrimmed_flameids = ['2.0276547153583627E-4', '2.1343733845877503E-4', '2.2467088258818426E-4', '2.3649566588229923E-4', '2.4894280619189394E-4', '2.6204505914936203E-4', '2.7583690436774953E-4', '2.903546361765785E-4', '3.056364591332405E-4', '3.2172258856130585E-4', '3.3865535638032194E-4', '0.0032353354497370902']\n",
    "\n",
    "framework_untrimmed_flame_key_ints = [int(float(framework_untrimmed_flameids[i])*10000000) for i in range(len(framework_untrimmed_flameids))]\n",
    "\n",
    "def isFlame_included(flame_key_int):\n",
    "    if flame_key_int in framework_untrimmed_flame_key_ints:\n",
    "        ret_val = 1\n",
    "    else:\n",
    "        ret_val = 0\n",
    "    return ret_val\n",
    "\n",
    "df['is_flame_included_by_framework'] = df['flame_key_int'].map(lambda x: isFlame_included(x))\n",
    "\n",
    "df['souener_deciles'] = pd.qcut(df['souener'],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ed1639a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_principal_components = 5\n",
    "\n",
    "pca = PCA(n_components=num_principal_components)\n",
    "\n",
    "icovariates = []\n",
    "for c in df.columns:\n",
    "    if c[0:2] == 'Yi':\n",
    "        icovariates.append(c)\n",
    "\n",
    "X = df[icovariates].values\n",
    "        \n",
    "pure_pca_dim_cols = [\"PURE_PCA_\"+str(i+1) for i in range(num_principal_components)]\n",
    "\n",
    "pca.fit_transform(X)\n",
    "        \n",
    "df_pure_pca = pd.DataFrame(pca.transform(X), columns = pure_pca_dim_cols)\n",
    "        \n",
    "df = pd.concat([df,df_pure_pca], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22898213",
   "metadata": {},
   "source": [
    "### Create PCAs orthogonal to Zmix and add to DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5c13ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zmix_pca_dim_cols = [\"Zmix_PCA_\"+str(i+1) for i in range(num_principal_components)]\n",
    "\n",
    "wopt = np.array([0.25131806468584, 1.0, 0.0, 0.0, 0.05926499970012948, 0.11189834407236524, 0.03053739933116691, 0.05926499970012948, 0.0, 0.07742283372149472, 0.14371856860332313, 0.14371856860332313, 0.20112514400193687, 1.0, 0.0, 0.0, 0.03473494419333629, 0.06713785861443991, 0.09743596683886535, 0.09743596683886535, 0.12582790137651187, 0.04027033873046593, 0.07742283372149472, 0.11180607885607882, 0.14371856860332313, 0.17341738612784788, 0.20112514400193687, 0.024566681794273966, 0.04795526192839207, 0.04795526192839207, 0.0, 0.06713048065088474, 0.12581494366075874, 0.17755300484072126, 0.034730994502665966, 0.0, 0.0, 0.0, 0.03249947443158002, 0.0, 0.0372961080230628, 0.07191024382448291, 0.024564706019978535, 0.023426986426879046, 0.023426986426879046, 0.023426986426879046, 0.0, 0.16374935944566987, 0.18286442054789118, 0.07024850027715426, 0.09152158240065958, 0.0, 0.0] , dtype=float)\n",
    "\n",
    "w = wopt[:,np.newaxis]\n",
    "\n",
    "# center the data\n",
    "Xcenter = X - np.mean(X)\n",
    "\n",
    "A = np.cov(X.T)\n",
    "\n",
    "# calculate A - ww^TA\n",
    "L = A - np.dot(np.dot(w,w.T),A)\n",
    "\n",
    "# get the first eigen vector\n",
    "values,vectors = np.linalg.eig(L)\n",
    "\n",
    "vectors = np.real(vectors)\n",
    "\n",
    "values = np.real(values)\n",
    "\n",
    "df_zmix_pca = pd.DataFrame(df['flame_key_int'], columns = ['flame_key_int'])\n",
    "\n",
    "df_zmix_pca[zmix_pca_dim_cols[0]] = Xcenter.dot(wopt)\n",
    "\n",
    "for i in range(len(zmix_pca_dim_cols)-1):\n",
    "    df_zmix_pca[zmix_pca_dim_cols[i+1]] = Xcenter.dot(vectors.T[i])\n",
    "\n",
    "df_zmix_pca = pd.DataFrame(pca.transform(X), columns = zmix_pca_dim_cols)\n",
    "        \n",
    "df = pd.concat([df,df_zmix_pca], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fecd566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_included_flames_int = df[df['is_flame_included_by_framework'] == 1]['flame_key_int'].unique()\n",
    "\n",
    "framework_excluded_flames_int = df[df['is_flame_included_by_framework'] == 0]['flame_key_int'].unique()\n",
    "\n",
    "all_flames_int = df['flame_key_int'].unique()\n",
    "\n",
    "def getTrainTestFlames(method):\n",
    "    \n",
    "    training_flames_int = []\n",
    "\n",
    "    testing_flames_int = []\n",
    "\n",
    "    if(method == \"frameworkincludedexcludedequalsplit\"):\n",
    "    \n",
    "        for x in framework_included_flames_int:\n",
    "            training_flames_int.append(x)\n",
    "\n",
    "        for x in framework_excluded_flames_int[::2]:\n",
    "            training_flames_int.append(x)\n",
    "\n",
    "        for x in framework_included_flames_int:\n",
    "            testing_flames_int.append(x)\n",
    "\n",
    "        for x in framework_excluded_flames_int[1::2]:\n",
    "            testing_flames_int.append(x)\n",
    "    \n",
    "    elif(method == \"frameworkincludedtrainexcludedtest\"):\n",
    "        for x in framework_included_flames_int:\n",
    "            training_flames_int.append(x)\n",
    "\n",
    "        for x in framework_excluded_flames_int:\n",
    "            testing_flames_int.append(x)\n",
    "    \n",
    "    elif(method == \"frameworkincludedtrainexcludedandincludedtest\"):\n",
    "        for x in framework_included_flames_int:\n",
    "            training_flames_int.append(x)\n",
    "\n",
    "        for x in framework_included_flames_int:\n",
    "            testing_flames_int.append(x)\n",
    "\n",
    "        for x in framework_excluded_flames_int:\n",
    "            testing_flames_int.append(x)\n",
    "            \n",
    "    else:\n",
    "        for x in all_flames_int:\n",
    "            training_flames_int.append(x)\n",
    "            testing_flames_int.append(x)\n",
    "    \n",
    "    df_training = df[df['flame_key_int'].isin(training_flames_int)]\n",
    "\n",
    "    df_testing = df[df['flame_key_int'].isin(testing_flames_int)]\n",
    "    \n",
    "    return df_training,df_testing\n",
    "\n",
    "\n",
    "def getTrainTestData(method):\n",
    "    \n",
    "    method_parts = method.split('_')\n",
    "\n",
    "    df_trainingFlames, df_testingFlames = getTrainTestFlames(method_parts[1])\n",
    "\n",
    "    if method_parts[0] == \"ZmixCpv\":\n",
    "        X_train = df_trainingFlames [[\"Zmix\",\"Cpv\"]].values\n",
    "        \n",
    "        Y_train = df_trainingFlames [[\"souener\"]].values\n",
    "        \n",
    "        X_test = df_testingFlames [[\"Zmix\",\"Cpv\"]].values\n",
    "        \n",
    "        Y_test = df_testingFlames [[\"souener\"]].values\n",
    "    \n",
    "    elif method_parts[0] == \"ZmixPCA\":\n",
    "        X_train = df_trainingFlames [zmix_pca_dim_cols].values\n",
    "        \n",
    "        Y_train = df_trainingFlames [[\"souener\"]].values\n",
    "        \n",
    "        X_test = df_testingFlames [zmix_pca_dim_cols].values\n",
    "        \n",
    "        Y_test = df_testingFlames [[\"souener\"]].values\n",
    "        \n",
    "    elif method_parts[0] == \"SparsePCA\":\n",
    "        X_train = df_trainingFlames [sparse_pca_dim_cols].values\n",
    "        \n",
    "        Y_train = df_trainingFlames [[\"souener\"]].values\n",
    "        \n",
    "        X_test = df_testingFlames [sparse_pca_dim_cols].values\n",
    "        \n",
    "        Y_test = df_testingFlames [[\"souener\"]].values\n",
    "    \n",
    "    elif method_parts[0] == \"PurePCA\":\n",
    "        X_train = df_trainingFlames [pure_pca_dim_cols].values\n",
    "        \n",
    "        Y_train = df_trainingFlames [[\"souener\"]].values\n",
    "        \n",
    "        X_test = df_testingFlames [pure_pca_dim_cols].values\n",
    "        \n",
    "        Y_test = df_testingFlames [[\"souener\"]].values\n",
    "        \n",
    "    else:\n",
    "        X_train = df_trainingFlames [icovariates].values\n",
    "        \n",
    "        Y_train = df_trainingFlames [[\"souener\"]].values\n",
    "        \n",
    "        X_test = df_testingFlames [icovariates].values\n",
    "        \n",
    "        Y_test = df_testingFlames [[\"souener\"]].values\n",
    "    \n",
    "    return X_train,Y_train,X_test,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7b8e3984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate data normalization with sklearn\n",
    "from sklearn.preprocessing import QuantileTransformer,MinMaxScaler\n",
    "\n",
    "def getHalfData(ipscaler=\"MinMaxScaler\",opscaler=\"MinMaxScaler\"):\n",
    "    X_allSpecies = df[icovariates].values\n",
    "    X= df[zmix_pca_dim_cols].values\n",
    "    Y = df[\"souener\"].values\n",
    "    Zmix = df[\"Zmix\"].values\n",
    "    \n",
    "    from sklearn.utils import shuffle\n",
    "    \n",
    "    X_allSpecies_shuffled,X_shuffled, Y_shuffled,Zmix_shuffled = shuffle(X_allSpecies,X,Y,Zmix, random_state=0)\n",
    "\n",
    "    X_allSpecies_train = X_allSpecies_shuffled[::2]\n",
    "    X_train = X_shuffled[::2]\n",
    "    Y_train = Y_shuffled[::2]\n",
    "    Zmix_train = Zmix_shuffled[::2]\n",
    "    \n",
    "    X_allSpecies_test = X_allSpecies_shuffled[1::2]\n",
    "    X_test = X_shuffled[1::2]\n",
    "    Y_test = Y_shuffled[1::2]\n",
    "    Zmix_test = Zmix_shuffled[1::2]\n",
    "\n",
    "    Y_train = Y_train.reshape(Y_train.shape[0], 1)\n",
    "    Y_test = Y_test.reshape(Y_test.shape[0], 1)\n",
    "\n",
    "    if ipscaler == \"MinMaxScaler\":\n",
    "        inputScalerFunction = MinMaxScaler\n",
    "    else:\n",
    "        inputScalerFunction = QuantileTransformer\n",
    "    \n",
    "    if opscaler == \"MinMaxScaler\":\n",
    "        outputScalerFunction = MinMaxScaler\n",
    "    else:\n",
    "        outputScalerFunction = QuantileTransformer\n",
    "    \n",
    "    \n",
    "    # create scaler\n",
    "    scaler_species = inputScalerFunction()\n",
    "    # fit and transform in one step\n",
    "    normalized_species_train = scaler_species.fit_transform(X_allSpecies_train)\n",
    "    normalized_species_test = scaler_species.fit_transform(X_allSpecies_test)\n",
    "\n",
    "    \n",
    "    # create scaler\n",
    "    scaler_zmixpca = inputScalerFunction()\n",
    "    \n",
    "    # fit and transform in one step\n",
    "    normalized_zmixpca_train = scaler_zmixpca.fit_transform(X_train)\n",
    "    normalized_zmixpca_test = scaler_zmixpca.fit_transform(X_test)\n",
    "\n",
    "    scaler_souener = outputScalerFunction()#MinMaxScaler()\n",
    "    \n",
    "    # fit and transform in one step\n",
    "    normalized_souener_train = scaler_souener.fit_transform(Y_train)\n",
    "    normalized_souener_test = scaler_souener.fit_transform(Y_test)\n",
    "\n",
    "    normalized_souener_train = normalized_souener_train.flatten()\n",
    "    normalized_souener_test = normalized_souener_test.flatten()\n",
    "    return {\"scaler_species\":scaler_species,\"scaler_zmixpca\":scaler_zmixpca,\"scaler_souener\":scaler_souener,\"X_tain\":X_train,\"X_test\":X_test,\"Zmix_train\":Zmix_train,\"Zmix_test\":Zmix_test,\"normalized_species_train\":normalized_species_train,\"normalized_species_test\":normalized_species_test,\"Y_test\":Y_test,\"Y_train\":Y_train,\"normalized_zmixpca_train\":normalized_zmixpca_train,\"normalized_zmixpca_test\":normalized_zmixpca_test,\"normalized_souener_train\": normalized_souener_train, \"normalized_souener_test\":normalized_souener_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c92e741",
   "metadata": {},
   "source": [
    "### Constrained DNN -- Baseline (Zmix + 4 Dim Linear Embedding; All Constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9bf22e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X_tain': array([[-2.13712999e-01, -2.13561222e-01, -2.55363508e-02,\n",
      "         1.39535261e-02,  9.99342274e-03],\n",
      "       [-1.99183858e-01, -2.88719669e-01,  2.23158718e-02,\n",
      "         8.42527814e-04,  1.70423532e-02],\n",
      "       [ 5.57799385e-01,  1.37512957e-01,  1.69304547e-04,\n",
      "        -1.03392923e-02,  7.54874232e-03],\n",
      "       ...,\n",
      "       [ 4.20009268e-01,  6.23979248e-02, -9.09985386e-04,\n",
      "        -3.40242182e-03,  2.69585883e-05],\n",
      "       [ 5.84461486e-01,  1.60704189e-01, -2.27067788e-02,\n",
      "         1.35496791e-03,  3.55269323e-03],\n",
      "       [-3.13848400e-01,  1.93695396e-01, -1.28770314e-01,\n",
      "        -1.98075447e-02, -1.84310336e-02]]), 'X_test': array([[ 7.65450206e-03, -1.12424729e-01, -3.04497110e-03,\n",
      "         3.51020996e-02, -1.67147857e-02],\n",
      "       [-4.99007241e-02, -1.40480713e-01, -6.74080092e-04,\n",
      "         3.81385554e-02, -1.83215458e-02],\n",
      "       [ 6.57392103e-01,  2.11461757e-01, -3.00280821e-02,\n",
      "         5.78369958e-03,  7.07989986e-03],\n",
      "       ...,\n",
      "       [-2.62419800e-01, -3.15944798e-02, -2.36864691e-02,\n",
      "         1.61996370e-02,  2.50027845e-02],\n",
      "       [ 5.02069519e-01,  1.14417143e-01, -1.77388972e-02,\n",
      "         2.07661877e-03,  3.63615925e-04],\n",
      "       [-3.26368224e-01,  2.29089043e-01, -5.24581312e-02,\n",
      "        -8.22687359e-03,  4.11399816e-04]]), 'Zmix_train': array([0.26717778, 0.29624862, 0.84587741, ..., 0.75384895, 0.85976993,\n",
      "       0.12650479]), 'Zmix_test': array([0.46907244, 0.4290207 , 0.90522953, ..., 0.19454853, 0.8057736 ,\n",
      "       0.12056065]), 'Y_test': array([[5.58632012e+09],\n",
      "       [4.02360148e+09],\n",
      "       [5.81072585e+08],\n",
      "       ...,\n",
      "       [7.87871399e+10],\n",
      "       [3.31228134e+08],\n",
      "       [1.02734466e+10]]), 'Y_train': array([[9.93219154e+10],\n",
      "       [1.73431242e+10],\n",
      "       [3.25476021e+05],\n",
      "       ...,\n",
      "       [1.24454277e+08],\n",
      "       [6.93820242e+07],\n",
      "       [2.18119232e+09]])}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "halfData = getHalfData()\n",
    "\n",
    "control_keys = ['X_tain', 'X_test', 'Y_train', 'Y_test', 'Zmix_train', 'Zmix_test']\n",
    "control_dict = {k:v for k,v in halfData.items() if k in control_keys}\n",
    "print(control_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "04340f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " species_input (InputLayer)     [(None, 53)]         0           []                               \n",
      "                                                                                                  \n",
      " Zmix (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 4)            216         ['species_input[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 5)            0           ['Zmix[0][0]',                   \n",
      "                                                                  'dense_30[0][0]']               \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 32)           192         ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " dense_32 (Dense)               (None, 64)           2112        ['dense_31[0][0]']               \n",
      "                                                                                                  \n",
      " dense_33 (Dense)               (None, 128)          8320        ['dense_32[0][0]']               \n",
      "                                                                                                  \n",
      " dense_34 (Dense)               (None, 256)          33024       ['dense_33[0][0]']               \n",
      "                                                                                                  \n",
      " dense_35 (Dense)               (None, 512)          131584      ['dense_34[0][0]']               \n",
      "                                                                                                  \n",
      " dense_36 (Dense)               (None, 256)          131328      ['dense_35[0][0]']               \n",
      "                                                                                                  \n",
      " dense_37 (Dense)               (None, 128)          32896       ['dense_36[0][0]']               \n",
      "                                                                                                  \n",
      " dense_38 (Dense)               (None, 64)           8256        ['dense_37[0][0]']               \n",
      "                                                                                                  \n",
      " dense_39 (Dense)               (None, 32)           2080        ['dense_38[0][0]']               \n",
      "                                                                                                  \n",
      " prediction (Dense)             (None, 1)            33          ['dense_39[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 350,041\n",
      "Trainable params: 350,041\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoding_dim = 4\n",
    "\n",
    "species_inputs = keras.Input(shape=(53,), name=\"species_input\")\n",
    "\n",
    "Zmix = keras.Input(shape=(1,), name=\"Zmix\")\n",
    "\n",
    "x = layers.Dense(encoding_dim, activation=\"linear\",kernel_constraint=UnitNorm(axis=0),kernel_regularizer=WeightsOrthogonalityConstraint(encoding_dim, weightage=1., axis=0),activity_regularizer=UncorrelatedFeaturesConstraint(encoding_dim, weightage=1.))(species_inputs)\n",
    "\n",
    "#Concatenate the Linear Embedding and Zmix together\n",
    "x = layers.Concatenate()([Zmix, x])\n",
    "\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dense(512, activation=\"relu\")(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "#Predict the source energy\n",
    "souener_pred = layers.Dense(1, name=\"prediction\")(x)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=[species_inputs,Zmix],\n",
    "    outputs=[souener_pred],\n",
    ")\n",
    "\n",
    "\n",
    "#,kernel_regularizer=WeightsOrthogonalityConstraint(encoding_dim, weightage=1., axis=0)\n",
    "#,kernel_constraint=UnitNorm(axis=0)\n",
    "#,activity_regularizer=UncorrelatedFeaturesConstraint(encoding_dim, weightage=1.)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7c20d16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model,to_file=\"model.png\",\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=False,\n",
    "    dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6df85a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_absolute_error',optimizer=tf.keras.optimizers.Adam(0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8e4223e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0078 - val_loss: 0.0051\n",
      "Epoch 2/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0081 - val_loss: 0.0124\n",
      "Epoch 3/100\n",
      "206/206 [==============================] - 2s 9ms/step - loss: 0.0094 - val_loss: 0.0114\n",
      "Epoch 4/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0080 - val_loss: 0.0058\n",
      "Epoch 5/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0070 - val_loss: 0.0047\n",
      "Epoch 6/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0066 - val_loss: 0.0058\n",
      "Epoch 7/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0068 - val_loss: 0.0060\n",
      "Epoch 8/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0081 - val_loss: 0.0063\n",
      "Epoch 9/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0084 - val_loss: 0.0054\n",
      "Epoch 10/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0079 - val_loss: 0.0077\n",
      "Epoch 11/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0059 - val_loss: 0.0071\n",
      "Epoch 12/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0076 - val_loss: 0.0065\n",
      "Epoch 13/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0064 - val_loss: 0.0067\n",
      "Epoch 14/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0076 - val_loss: 0.0047\n",
      "Epoch 15/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0065 - val_loss: 0.0051\n",
      "Epoch 16/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0063 - val_loss: 0.0064\n",
      "Epoch 17/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0073 - val_loss: 0.0097\n",
      "Epoch 18/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0060 - val_loss: 0.0065\n",
      "Epoch 19/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0059 - val_loss: 0.0056\n",
      "Epoch 20/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0059 - val_loss: 0.0061\n",
      "Epoch 21/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0071 - val_loss: 0.0072\n",
      "Epoch 22/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0069 - val_loss: 0.0054\n",
      "Epoch 23/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0060 - val_loss: 0.0054\n",
      "Epoch 24/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0063 - val_loss: 0.0069\n",
      "Epoch 25/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 26/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 27/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0061 - val_loss: 0.0047\n",
      "Epoch 28/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0063 - val_loss: 0.0050\n",
      "Epoch 29/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 30/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0059 - val_loss: 0.0068\n",
      "Epoch 31/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0066 - val_loss: 0.0068\n",
      "Epoch 32/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0057 - val_loss: 0.0044\n",
      "Epoch 33/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 34/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0060 - val_loss: 0.0067\n",
      "Epoch 35/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0061 - val_loss: 0.0069\n",
      "Epoch 36/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0060 - val_loss: 0.0066\n",
      "Epoch 37/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0064 - val_loss: 0.0057\n",
      "Epoch 38/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0055 - val_loss: 0.0059\n",
      "Epoch 39/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0060 - val_loss: 0.0062\n",
      "Epoch 40/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0054 - val_loss: 0.0048\n",
      "Epoch 41/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0070 - val_loss: 0.0058\n",
      "Epoch 42/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0077 - val_loss: 0.0056\n",
      "Epoch 43/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0059 - val_loss: 0.0043\n",
      "Epoch 44/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0057 - val_loss: 0.0040\n",
      "Epoch 45/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0053 - val_loss: 0.0058\n",
      "Epoch 46/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0057 - val_loss: 0.0045\n",
      "Epoch 47/100\n",
      "206/206 [==============================] - 2s 9ms/step - loss: 0.0053 - val_loss: 0.0050\n",
      "Epoch 48/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0052 - val_loss: 0.0048\n",
      "Epoch 49/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0058 - val_loss: 0.0099\n",
      "Epoch 50/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0058 - val_loss: 0.0035\n",
      "Epoch 51/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0056 - val_loss: 0.0070\n",
      "Epoch 52/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0068 - val_loss: 0.0110\n",
      "Epoch 53/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0072 - val_loss: 0.0074\n",
      "Epoch 54/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0066 - val_loss: 0.0088\n",
      "Epoch 55/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0067 - val_loss: 0.0039\n",
      "Epoch 56/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0058 - val_loss: 0.0041\n",
      "Epoch 57/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0052 - val_loss: 0.0086\n",
      "Epoch 58/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0061 - val_loss: 0.0084\n",
      "Epoch 59/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0051 - val_loss: 0.0042\n",
      "Epoch 60/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0055 - val_loss: 0.0079\n",
      "Epoch 61/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0052 - val_loss: 0.0048\n",
      "Epoch 62/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0052 - val_loss: 0.0038\n",
      "Epoch 63/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0050 - val_loss: 0.0038\n",
      "Epoch 64/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0051 - val_loss: 0.0056\n",
      "Epoch 65/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0053 - val_loss: 0.0060\n",
      "Epoch 66/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0043 - val_loss: 0.0068\n",
      "Epoch 67/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0054 - val_loss: 0.0036\n",
      "Epoch 68/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0052 - val_loss: 0.0048\n",
      "Epoch 69/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0064 - val_loss: 0.0088\n",
      "Epoch 70/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0070 - val_loss: 0.0044\n",
      "Epoch 71/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0067 - val_loss: 0.0076\n",
      "Epoch 72/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0055 - val_loss: 0.0052\n",
      "Epoch 73/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0059 - val_loss: 0.0037\n",
      "Epoch 74/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0053 - val_loss: 0.0055\n",
      "Epoch 75/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0051 - val_loss: 0.0040\n",
      "Epoch 76/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0049 - val_loss: 0.0048\n",
      "Epoch 77/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0049 - val_loss: 0.0099\n",
      "Epoch 78/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0064 - val_loss: 0.0048\n",
      "Epoch 79/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0051 - val_loss: 0.0055\n",
      "Epoch 80/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0046 - val_loss: 0.0039\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0063 - val_loss: 0.0089\n",
      "Epoch 82/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0071 - val_loss: 0.0068\n",
      "Epoch 83/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0067 - val_loss: 0.0076\n",
      "Epoch 84/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0062 - val_loss: 0.0064\n",
      "Epoch 85/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0065 - val_loss: 0.0085\n",
      "Epoch 86/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0066 - val_loss: 0.0056\n",
      "Epoch 87/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0063 - val_loss: 0.0057\n",
      "Epoch 88/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0048 - val_loss: 0.0041\n",
      "Epoch 89/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0057 - val_loss: 0.0059\n",
      "Epoch 90/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0045 - val_loss: 0.0039\n",
      "Epoch 91/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0044 - val_loss: 0.0037\n",
      "Epoch 92/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0051 - val_loss: 0.0068\n",
      "Epoch 93/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0049 - val_loss: 0.0056\n",
      "Epoch 94/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 95/100\n",
      "206/206 [==============================] - 2s 8ms/step - loss: 0.0047 - val_loss: 0.0056\n",
      "Epoch 00095: early stopping\n",
      "CPU times: user 4min 3s, sys: 23.3 s, total: 4min 26s\n",
      "Wall time: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "normalized_species_train = halfData[\"normalized_species_train\"]\n",
    "Zmix_train = halfData[\"Zmix_train\"] \n",
    "normalized_souener_train = halfData[\"normalized_souener_train\"]\n",
    "\n",
    "history = model.fit([normalized_species_train,Zmix_train], \n",
    "                          normalized_souener_train,\n",
    "                          validation_split=0.2,\n",
    "                          verbose=1, \n",
    "                          epochs=100, \n",
    "                          callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "34d6761d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-a5d8c9c0dab5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_loss' is not defined"
     ]
    }
   ],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b199da",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_species_test = halfData[\"normalized_species_test\"]\n",
    "\n",
    "Zmix_test =  halfData[\"Zmix_test\"]\n",
    "\n",
    "predictions = model.predict([normalized_species_test,Zmix_test])\n",
    "\n",
    "normalized_souener_pred = predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05486bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_souener = halfData[\"scaler_souener\"]\n",
    "Y_pred = scaler_souener.inverse_transform(normalized_souener_pred)\n",
    "Y_pred = Y_pred.flatten()\n",
    "Y_test = halfData[\"Y_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = computeAndPrintError(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4cf094",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.residplot(Y_test.flatten(), getResiduals(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beffbfe",
   "metadata": {},
   "source": [
    "## RESULTS SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e37865",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>Method </td>\n",
    "<td>MAE</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Framework </td>\n",
    "<td>2.24 E+09</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>DNN (Non-Linear Embedding) </td>\n",
    "<td>9.304 E+08</td>\n",
    "</tr>    \n",
    "<tr>\n",
    "<td>DNN (Linear Embedding [No Constraints]) </td>\n",
    "<td>2.042 E+09</td>\n",
    "</tr>       \n",
    "<tr>\n",
    "<td>ChemTab (Unit Norm Constraints) </td>\n",
    "<td>6.28 E+08</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ChemTab (Orthogonal Weights Constraints) </td>\n",
    "<td>6.05 E+08</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ChemTab (Uncorrelated Features Constraints) </td>\n",
    "<td>5.7 E+08</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ChemTab (All Constraints) </td>\n",
    "<td>8.008 E+08</td>\n",
    "</tr>\n",
    "    \n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
